{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8564a9a9",
   "metadata": {},
   "source": [
    "# Paired integration tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c03d6",
   "metadata": {},
   "source": [
    "In this notebook we showcase basic Multigrate functionality and integrate a CITE-seq dataset to obtain a joint gene expression and protein abundance representation in a latent space. We use publically available dataset from NeurIPS 2021 workshop https://openproblems.bio/neurips_2021/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54c03b",
   "metadata": {},
   "source": [
    "The preprocessed data can be downloaded from https://drive.google.com/drive/folders/1SJeo1OYM__h8vB6dXzGWC8FcLV5fUmgZ?usp=sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ae01e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "/Users/ilangold/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/umap/__init__.py:9: ImportWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import multigrate as mtg\n",
    "import scib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import anndata\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import pandas as pd\n",
    "from muon import prot as pt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "guided-supplement",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "### CHANGE THESE PARAMETERS TO WORK ON REAL DATA OR TEST DATA ###\n",
    "#################################################################\n",
    "cite_batch_key = \"COMBAT_participant_timepoint_ID\"  # Used in various places\n",
    "cytof_batch_key = \"COMBAT_ID_Time\"  # Used in various places\n",
    "batch_key = \"COMBAT_participant_timepoint_ID\"  # a shared id\n",
    "use_curated_hvg = True  # Whether or not to use the curated HVG all_HVG_by_ct.tsv\n",
    "correlation_removal_mode = \"low\"\n",
    "correlation_removal_percentage = 80\n",
    "# Comment in/out which configuration you wish to use.\n",
    "\n",
    "# TESTING:\n",
    "models_path_str = \"../models\"\n",
    "models_path = Path(models_path_str)\n",
    "models_path.mkdir(parents=True, exist_ok=True)\n",
    "data_path_str = \"../../../../scfglue/tests/fixtures\"\n",
    "data_path = Path(data_path_str)\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "adata_cite = anndata.read_h5ad(str(data_path / 'cite-seq.h5ad'))\n",
    "adata_cytof = anndata.read_h5ad(str(data_path / 'cytof.h5ad'))\n",
    "adata_cytof.obs[batch_key] = adata_cytof.obs[cytof_batch_key]  # For shared batches\n",
    "n_comps = 20  # Used for PCA calculation, lower than the number of proteins presetn\n",
    "scglue_kwargs = {\"max_epochs\": 50}  # Limit the number of epochs\n",
    "shared_batches = True  # should we only use overlapping batches for real?\n",
    "remove_low_count_batches = False\n",
    "file_suffix = \"\"\n",
    "use_subset = False\n",
    "hvg_list_path = \"../../../../scfglue/notebooks/all_HVG_by_ct.tsv\"\n",
    "\n",
    "# TRUE RUN:\n",
    "# models_path_str = \"/n/scratch3/users/i/ig62/models/multigrate/subset_model/\"\n",
    "# data_path_str = \"/n/scratch3/users/i/ig62/data/\"\n",
    "# file_suffix = \"no_integration\"\n",
    "# hvg_list_path = \"../../../../scfglue/notebooks/all_HVG_by_ct.tsv\"\n",
    "# n_comps = 100\n",
    "# use_subset = True\n",
    "\n",
    "# scglue_kwargs = {\"max_epochs\": -1}  # Allow scGLUE to determine epochs\n",
    "# shared_batches = True  # should we only use overlapping batches?\n",
    "# remove_low_count_batches = False  # Need this for large datasets with low cell-count batches if you use the standard HVG pipeline (not the included file as filter): https://github.com/scverse/scanpy/issues/1504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f14506",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = Path(models_path_str)\n",
    "models_path.mkdir(parents=True, exist_ok=True)\n",
    "data_path = Path(data_path_str)\n",
    "\n",
    "adata_cite = anndata.read_h5ad(str(data_path / 'cite-seq.h5ad'))\n",
    "adata_cytof = anndata.read_h5ad(str(data_path / 'cytof.h5ad'))\n",
    "if use_subset:\n",
    "    # subset the data\n",
    "    # Only using initial time points\n",
    "    subset = [\n",
    "        # Two COVID_SEV\n",
    "        'S00028-Ja001',\n",
    "        'S00033-Ja001',\n",
    "        # Two COVID_MILD\n",
    "        'S00002-Ja001',\n",
    "        'S00016-Ja001',\n",
    "        # Two COVID_CRIT\n",
    "        'S00050-Ja001',\n",
    "        'S00054-Ja001',\n",
    "        # Two Sepsis\n",
    "        'N00021-Ja001',\n",
    "        'N00023-Ja001',\n",
    "        # Two HV\n",
    "        'H00072-Ha001',\n",
    "        'H00085-Ha001',\n",
    "        # No Flu - not present in CYTOF\n",
    "    ]\n",
    "    adata_cytof = adata_cytof[adata_cytof.obs[cytof_batch_key].isin(subset), :]\n",
    "    adata_cite = adata_cite[adata_cite.obs[cite_batch_key].isin(subset), :]\n",
    "\n",
    "adata_cytof.obs[batch_key] = adata_cytof.obs[cytof_batch_key]  # For shared batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c98650",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_mask = adata_cite.var['feature_types'] == 'Gene Expression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2507bf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 409 × 55\n",
       "    obs: 'Annotation_cluster_id', 'Annotation_cluster_name', 'Annotation_minor_subset', 'Annotation_major_subset', 'Annotation_cell_type', 'GEX_region', 'QC_ngenes', 'QC_total_UMI', 'QC_pct_mitochondrial', 'QC_scrub_doublet_scores', 'TCR_chain_composition', 'TCR_clone_ID', 'TCR_clone_count', 'TCR_clone_proportion', 'TCR_contains_unproductive', 'TCR_doublet', 'TCR_chain_TRA', 'TCR_v_gene_TRA', 'TCR_d_gene_TRA', 'TCR_j_gene_TRA', 'TCR_c_gene_TRA', 'TCR_productive_TRA', 'TCR_cdr3_TRA', 'TCR_umis_TRA', 'TCR_chain_TRA2', 'TCR_v_gene_TRA2', 'TCR_d_gene_TRA2', 'TCR_j_gene_TRA2', 'TCR_c_gene_TRA2', 'TCR_productive_TRA2', 'TCR_cdr3_TRA2', 'TCR_umis_TRA2', 'TCR_chain_TRB', 'TCR_v_gene_TRB', 'TCR_d_gene_TRB', 'TCR_j_gene_TRB', 'TCR_c_gene_TRB', 'TCR_productive_TRB', 'TCR_chain_TRB2', 'TCR_v_gene_TRB2', 'TCR_d_gene_TRB2', 'TCR_j_gene_TRB2', 'TCR_c_gene_TRB2', 'TCR_productive_TRB2', 'TCR_cdr3_TRB2', 'TCR_umis_TRB2', 'BCR_umis_HC', 'BCR_contig_qc_HC', 'BCR_functionality_HC', 'BCR_v_call_HC', 'BCR_v_score_HC', 'BCR_j_call_HC', 'BCR_j_score_HC', 'BCR_junction_aa_HC', 'BCR_total_mut_HC', 'BCR_s_mut_HC', 'BCR_r_mut_HC', 'BCR_c_gene_HC', 'BCR_clone_per_replicate_HC', 'BCR_clone_global_HC', 'BCR_clonal_abundance_HC', 'BCR_locus_LC', 'BCR_umis_LC', 'BCR_contig_qc_LC', 'BCR_functionality_LC', 'BCR_v_call_LC', 'BCR_v_score_LC', 'BCR_j_call_LC', 'BCR_j_score_LC', 'BCR_junction_aa_LC', 'BCR_total_mut_LC', 'BCR_s_mut_LC', 'BCR_r_mut_LC', 'BCR_c_gene_LC', 'COMBAT_ID', 'scRNASeq_sample_ID', 'COMBAT_participant_timepoint_ID', 'Source', 'Age', 'Sex', 'Race', 'BMI', 'Hospitalstay', 'Death28', 'Institute', 'PreExistingHeartDisease', 'PreExistingLungDisease', 'PreExistingKidneyDisease', 'PreExistingDiabetes', 'PreExistingHypertension', 'PreExistingImmunocompromised', 'Smoking', 'Symptomatic', 'Requiredvasoactive', 'Respiratorysupport', 'SARSCoV2PCR', 'Outcome', 'TimeSinceOnset', 'Ethnicity', 'Tissue', 'DiseaseClassification', 'Pool_ID', 'Channel_ID'\n",
       "    var: 'gene_ids', 'feature_types'\n",
       "    uns: 'Institute', 'ObjectCreateDate', 'Source_colors', 'Technology', 'genome_annotation_version'\n",
       "    obsm: 'X_umap', 'X_umap_source'\n",
       "    layers: 'raw'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rna = adata_cite[:, genes_mask]\n",
    "rna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a71e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 409 × 36\n",
       "    obs: 'Annotation_cluster_id', 'Annotation_cluster_name', 'Annotation_minor_subset', 'Annotation_major_subset', 'Annotation_cell_type', 'GEX_region', 'QC_ngenes', 'QC_total_UMI', 'QC_pct_mitochondrial', 'QC_scrub_doublet_scores', 'TCR_chain_composition', 'TCR_clone_ID', 'TCR_clone_count', 'TCR_clone_proportion', 'TCR_contains_unproductive', 'TCR_doublet', 'TCR_chain_TRA', 'TCR_v_gene_TRA', 'TCR_d_gene_TRA', 'TCR_j_gene_TRA', 'TCR_c_gene_TRA', 'TCR_productive_TRA', 'TCR_cdr3_TRA', 'TCR_umis_TRA', 'TCR_chain_TRA2', 'TCR_v_gene_TRA2', 'TCR_d_gene_TRA2', 'TCR_j_gene_TRA2', 'TCR_c_gene_TRA2', 'TCR_productive_TRA2', 'TCR_cdr3_TRA2', 'TCR_umis_TRA2', 'TCR_chain_TRB', 'TCR_v_gene_TRB', 'TCR_d_gene_TRB', 'TCR_j_gene_TRB', 'TCR_c_gene_TRB', 'TCR_productive_TRB', 'TCR_chain_TRB2', 'TCR_v_gene_TRB2', 'TCR_d_gene_TRB2', 'TCR_j_gene_TRB2', 'TCR_c_gene_TRB2', 'TCR_productive_TRB2', 'TCR_cdr3_TRB2', 'TCR_umis_TRB2', 'BCR_umis_HC', 'BCR_contig_qc_HC', 'BCR_functionality_HC', 'BCR_v_call_HC', 'BCR_v_score_HC', 'BCR_j_call_HC', 'BCR_j_score_HC', 'BCR_junction_aa_HC', 'BCR_total_mut_HC', 'BCR_s_mut_HC', 'BCR_r_mut_HC', 'BCR_c_gene_HC', 'BCR_clone_per_replicate_HC', 'BCR_clone_global_HC', 'BCR_clonal_abundance_HC', 'BCR_locus_LC', 'BCR_umis_LC', 'BCR_contig_qc_LC', 'BCR_functionality_LC', 'BCR_v_call_LC', 'BCR_v_score_LC', 'BCR_j_call_LC', 'BCR_j_score_LC', 'BCR_junction_aa_LC', 'BCR_total_mut_LC', 'BCR_s_mut_LC', 'BCR_r_mut_LC', 'BCR_c_gene_LC', 'COMBAT_ID', 'scRNASeq_sample_ID', 'COMBAT_participant_timepoint_ID', 'Source', 'Age', 'Sex', 'Race', 'BMI', 'Hospitalstay', 'Death28', 'Institute', 'PreExistingHeartDisease', 'PreExistingLungDisease', 'PreExistingKidneyDisease', 'PreExistingDiabetes', 'PreExistingHypertension', 'PreExistingImmunocompromised', 'Smoking', 'Symptomatic', 'Requiredvasoactive', 'Respiratorysupport', 'SARSCoV2PCR', 'Outcome', 'TimeSinceOnset', 'Ethnicity', 'Tissue', 'DiseaseClassification', 'Pool_ID', 'Channel_ID'\n",
       "    var: 'gene_ids', 'feature_types'\n",
       "    uns: 'Institute', 'ObjectCreateDate', 'Source_colors', 'Technology', 'genome_annotation_version'\n",
       "    obsm: 'X_umap', 'X_umap_source'\n",
       "    layers: 'raw'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvg_list = pd.read_csv(Path(hvg_list_path), sep='\\t')['gene_name'].values.tolist()\n",
    "def filter_genes(adata, **kwargs):\n",
    "    adata = adata[:, adata.var.index.isin(hvg_list)]\n",
    "    return adata\n",
    "rna = filter_genes(rna)\n",
    "rna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25695280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0, ..., 408, 408, 408], dtype=int32),\n",
       " array([10, 20, 21, ..., 10, 20, 21], dtype=int32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adt = adata_cite[:, ~genes_mask]\n",
    "mat = adt.X.copy()\n",
    "mat.data = np.nan_to_num(mat.data,copy=False)\n",
    "adt.X = mat\n",
    "pt.pp.clr(adt)\n",
    "adt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491dd3d",
   "metadata": {},
   "source": [
    "## Prep the input AnnData object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00844ec0",
   "metadata": {},
   "source": [
    "First, we need to organize anndatas correspoding to different datasets and modalities into 1 anndata object. In this example we have 1 CITE-seq dataset, hence we input 2 anndata objects, 1 for RNA modality, 1 for ADT modality, and specify that they are paired (```groups``` argument).\n",
    "\n",
    "Notes:\n",
    "- paired datasets have to have the same .obs_names, i.e. index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6938906",
   "metadata": {},
   "source": [
    "A couple of examples on how to initialize the data and the model:\n",
    "- each sublist in `adatas`, `groups`, `layers` parameters correspond to one modality. If you have multiple objects per modality, append them to the corresponding list. If you have paired and unpaired measurements, the missing data should be registered with `None` (see examples below)\n",
    "- objects in each sublist *have* to have the same set of features: if you want to integrate multiple RNA objects, we recommend first concatenating full objects and then subsetting to 3000-4000 highly variable genes; for ADT modality, we take the intersection of available proteins (double check the naming conventions, as that can vary a lot from one dataset to another, so `.var_names` can have almost no intersection but if you align the protein names, then there is an overlap)\n",
    "- one paired CITE-seq experiment:\n",
    "```\n",
    "adatas = [[rna], [adt]],\n",
    "groups = [[0], [0]],\n",
    "```\n",
    "- one paired CITE-seq experiment and another RNA dataset:\n",
    "```\n",
    "adatas = [[rna_cite, rna], [adt_cite, None]],\n",
    "groups = [[0, 1], [0, 1]],\n",
    "layers = [['counts', None], [None, None]]\n",
    "```\n",
    "- `layers` parameter specifies which layer the model should take the counts from. If `None`, then defaults to `.X`. The distribution of the input data should be the same within a modality, i.e. either use raw counts for all RNA onjects or all normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3add7cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that obs_names are indeed the same\n",
    "np.sum(rna.obs_names != adt.obs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c1e546",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 981 × 66\n",
       "    obs: 'Annotation_cluster_id', 'Annotation_cluster_name', 'Annotation_minor_subset', 'Annotation_major_subset', 'Annotation_cell_type', 'GEX_region', 'QC_ngenes', 'QC_total_UMI', 'QC_pct_mitochondrial', 'QC_scrub_doublet_scores', 'TCR_chain_composition', 'TCR_clone_ID', 'TCR_clone_count', 'TCR_clone_proportion', 'TCR_contains_unproductive', 'TCR_doublet', 'TCR_chain_TRA', 'TCR_v_gene_TRA', 'TCR_d_gene_TRA', 'TCR_j_gene_TRA', 'TCR_c_gene_TRA', 'TCR_productive_TRA', 'TCR_cdr3_TRA', 'TCR_umis_TRA', 'TCR_chain_TRA2', 'TCR_v_gene_TRA2', 'TCR_d_gene_TRA2', 'TCR_j_gene_TRA2', 'TCR_c_gene_TRA2', 'TCR_productive_TRA2', 'TCR_cdr3_TRA2', 'TCR_umis_TRA2', 'TCR_chain_TRB', 'TCR_v_gene_TRB', 'TCR_d_gene_TRB', 'TCR_j_gene_TRB', 'TCR_c_gene_TRB', 'TCR_productive_TRB', 'TCR_chain_TRB2', 'TCR_v_gene_TRB2', 'TCR_d_gene_TRB2', 'TCR_j_gene_TRB2', 'TCR_c_gene_TRB2', 'TCR_productive_TRB2', 'TCR_cdr3_TRB2', 'TCR_umis_TRB2', 'BCR_umis_HC', 'BCR_contig_qc_HC', 'BCR_functionality_HC', 'BCR_v_call_HC', 'BCR_v_score_HC', 'BCR_j_call_HC', 'BCR_j_score_HC', 'BCR_junction_aa_HC', 'BCR_total_mut_HC', 'BCR_s_mut_HC', 'BCR_r_mut_HC', 'BCR_c_gene_HC', 'BCR_clone_per_replicate_HC', 'BCR_clone_global_HC', 'BCR_clonal_abundance_HC', 'BCR_locus_LC', 'BCR_umis_LC', 'BCR_contig_qc_LC', 'BCR_functionality_LC', 'BCR_v_call_LC', 'BCR_v_score_LC', 'BCR_j_call_LC', 'BCR_j_score_LC', 'BCR_junction_aa_LC', 'BCR_total_mut_LC', 'BCR_s_mut_LC', 'BCR_r_mut_LC', 'BCR_c_gene_LC', 'COMBAT_ID', 'scRNASeq_sample_ID', 'COMBAT_participant_timepoint_ID', 'Source', 'Age', 'Sex', 'Race', 'BMI', 'Hospitalstay', 'Death28', 'Institute', 'PreExistingHeartDisease', 'PreExistingLungDisease', 'PreExistingKidneyDisease', 'PreExistingDiabetes', 'PreExistingHypertension', 'PreExistingImmunocompromised', 'Smoking', 'Symptomatic', 'Requiredvasoactive', 'Respiratorysupport', 'SARSCoV2PCR', 'Outcome', 'TimeSinceOnset', 'Ethnicity', 'Tissue', 'DiseaseClassification', 'Pool_ID', 'Channel_ID', 'group', 'sample_id', 'condition', 'patient_id', 'batch', 'cellID', 'COMBAT_ID_Time', 'CyTOF_priority', 'major_cell_type', 'fine_cluster_id', 'concat_batch'\n",
       "    var: 'gene_ids-0', 'feature_types-0', 'channel_name-1', 'marker_name-1', 'marker_class-1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_proteins = list(set(adt.var_names).intersection(set(adata_cytof.var_names)))\n",
    "adt_common = adt[:, adt.var.index[adt.var.index.isin(common_proteins)]]\n",
    "adt_unique = adt[:, adt.var.index[~adt.var.index.isin(common_proteins)]]\n",
    "cytof_common = adata_cytof[:, adata_cytof.var.index[adata_cytof.var.index.isin(common_proteins)]]\n",
    "modality_lengths = [rna.shape[1], adt_common.shape[1], adt_unique.shape[1]]\n",
    "adatas = [[rna.copy(), None], [adt_common.copy(), cytof_common.copy()], [adt_unique.copy(), None]]\n",
    "groups = [[0, 1], [0, 1], [0, 1]]\n",
    "\n",
    "adata = mtg.data.organize_multiome_anndatas(\n",
    "    adatas = adatas,           # a list of anndata objects per modality, RNA-seq always goes first\n",
    "    groups = groups,               # groups that specify which anndatas are paired\n",
    "    modality_lengths = modality_lengths # how many features in each modality\n",
    ")\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c3b1e",
   "metadata": {},
   "source": [
    "From now on, we work with one concatenated anndata object ```adata```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4483c3",
   "metadata": {},
   "source": [
    "If using raw counts for scRNA, we need to use NB loss (or ZINB), thus need to calculate `size_factors` first. If using normalized counts and MSE for scRNA, `rna_indices_end` does not need to be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dae30e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m No batch_key inputted, assuming all cells are same batch                                                  \n",
      "\u001b[34mINFO    \u001b[0m No label_key inputted, assuming all cells have same label                                                 \n",
      "\u001b[34mINFO    \u001b[0m Using data from adata.X                                                                                   \n",
      "\u001b[34mINFO    \u001b[0m Successfully registered anndata object containing \u001b[1;36m981\u001b[0m cells, \u001b[1;36m66\u001b[0m vars, \u001b[1;36m1\u001b[0m batches, \u001b[1;36m1\u001b[0m labels, and \u001b[1;36m0\u001b[0m proteins.\n",
      "         Also registered \u001b[1;36m1\u001b[0m extra categorical covariates and \u001b[1;36m1\u001b[0m extra continuous covariates.                         \n",
      "\u001b[34mINFO    \u001b[0m Please do not further modify adata until model is trained.                                                \n"
     ]
    }
   ],
   "source": [
    "mtg.model.MultiVAE.setup_anndata(\n",
    "    adata,\n",
    "    rna_indices_end=rna.shape[1], # how many features in the rna-seq modality\n",
    "    categorical_covariate_keys=[batch_key]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d759a74",
   "metadata": {},
   "source": [
    "If categorical (e.g. sex) or continious covariates (e.g. age) are available, and you'd like to learn a latent representation disentangled from these covariates, additionally specify in setup_anndata\n",
    "```\n",
    "categorical_covariate_keys: [\"batch\"],\n",
    "continuous_covariate_keys: [\"age\"]\n",
    "```\n",
    "\n",
    "Here we want to get rid of batch effect, so we specify  `batch` as a categorical covariate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c57c1",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af207f",
   "metadata": {},
   "source": [
    "Define the model here. If using raw counts for RNA-seq, use NB loss, if normalized counts, use MSE. For ADT we use CLR-normalized counts and MSE loss.\n",
    "\n",
    "Usually it is not necessary to specify `integrate_on=\"batch\"` and the model correct for batch effects by learning the latent representation disentangled from the batch. If this fails, then we recommend using `integrate_on` parameter.\n",
    "\n",
    "We recommend setting `n_layers_encoders` and `n_layers_decoders` to 2 for each modality. Here it's set to 1 to minimize the training time.\n",
    "\n",
    "In case of the mosaic integration, it might be benefitial to set different weights to different modalities. We recommend first training the model with default equal weights (all equal to `1.0`), then checking the training curves (see below) and then if one of the modalities seem to overpower another (e.g. if one of the loss curves goes down but the other one doesn't) to balance the coefficients accordingly. If there are two modalities `0` and `1` (according to the order in which they were initialized), set the corresponding coeffitients as \n",
    "```\n",
    "loss_coefs = {\n",
    "   '0': 1.0,\n",
    "   '1': 0.001\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8cb4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mtg.model.MultiVAE(\n",
    "    adata, \n",
    "    modality_lengths=modality_lengths, # how many features per modality\n",
    "    losses=['nb', 'mse', 'mse'],         # what losses to use for each modality\n",
    "#     loss_coefs={\n",
    "#         'kl': 1e-5,\n",
    "#     }, # 'integ' coef is only used if integrate_on is used\n",
    "#     integrate_on=batch_key,\n",
    "    z_dim=15,\n",
    "    cond_dim=10,\n",
    "    dropout=0.2,\n",
    "    n_layers_encoders=[2, 2, 2],\n",
    "    n_layers_decoders=[2, 2, 2],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123063e",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03b6c1",
   "metadata": {},
   "source": [
    "Can specify the number of epochs by setting `max_epochs` parameter, default is 500. The default batch size is set to `batch_size = 128`, adjust if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f248b740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|                                                                                                                                                                                   | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (128, 15)) of distribution Normal(loc: torch.Size([128, 15]), scale: torch.Size([128, 15])) to satisfy the constraint Real(), but found invalid values:\ntensor([[ 0.2573,  0.1445, -0.2776,  ...,  0.2039,  0.2587,  0.2058],\n        [ 0.2999,  0.0161, -0.6274,  ...,  0.1477,  0.4026,  0.2286],\n        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n        ...,\n        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n        [ 0.3256,  0.1833, -0.0688,  ...,  0.1692,  0.2093,  0.1883]],\n       grad_fn=<MulBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/multigrate/model/_multivae.py:307\u001b[0m, in \u001b[0;36mMultiVAE.train\u001b[0;34m(self, max_epochs, lr, use_gpu, train_size, validation_size, batch_size, weight_decay, eps, early_stopping, save_best, check_val_every_n_epoch, n_steps_kl_warmup, plan_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m training_plan \u001b[38;5;241m=\u001b[39m MultiVAETrainingPlan(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mplan_kwargs)\n\u001b[1;32m    298\u001b[0m runner \u001b[38;5;241m=\u001b[39m TrainRunner(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    300\u001b[0m     training_plan\u001b[38;5;241m=\u001b[39mtraining_plan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    306\u001b[0m )\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/scvi/train/_trainrunner.py:72\u001b[0m, in \u001b[0;36mTrainRunner.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_splitter, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_train\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_plan\u001b[38;5;241m.\u001b[39mn_obs_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_splitter\u001b[38;5;241m.\u001b[39mn_train\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_plan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_splitter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# data splitter only gets these attrs after fit\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/scvi/train/_trainer.py:177\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], PyroTrainingPlan):\n\u001b[1;32m    172\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[1;32m    173\u001b[0m         action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    175\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`LightningModule.configure_optimizers` returned `None`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    176\u001b[0m     )\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:460\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# links data to the trainer\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    457\u001b[0m     model, train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    458\u001b[0m )\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:758\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch()\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_dispatch()\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:799\u001b[0m, in \u001b[0;36mTrainer.dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstart_predicting(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:96\u001b[0m, in \u001b[0;36mAccelerator.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:144\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_predict()\n\u001b[0;32m--> 809\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:871\u001b[0m, in \u001b[0;36mTrainer.run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loop\u001b[38;5;241m.\u001b[39mon_train_epoch_start(epoch)\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# run train epoch\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step:\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loop\u001b[38;5;241m.\u001b[39mon_train_end()\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py:499\u001b[0m, in \u001b[0;36mTrainLoop.run_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# ------------------------------------\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# TRAINING_STEP + TRAINING_STEP_END\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# ------------------------------------\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 499\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# when returning -1 from train_step, we end epoch early\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_output\u001b[38;5;241m.\u001b[39msignal \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py:738\u001b[0m, in \u001b[0;36mTrainLoop.run_training_batch\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# optimizer step\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;66;03m# revert back to previous state\u001b[39;00m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39muntoggle_optimizer(opt_idx)\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py:434\u001b[0m, in \u001b[0;36mTrainLoop.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    431\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m LightningOptimizer\u001b[38;5;241m.\u001b[39m_to_lightning_optimizer(optimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, opt_idx)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 434\u001b[0m \u001b[43mmodel_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_tpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDeviceType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTPU\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_TPU_AVAILABLE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_native_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_native_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_lbfgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_lbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py:1403\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1332\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1339\u001b[0m     using_lbfgs: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1340\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;124;03m    Override this method to adjust the default way the\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;124;03m    :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \n\u001b[1;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1403\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:214\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen closure is provided, it should be a function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m     profiler_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_step_and_closure_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofiler_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_optimizer_step_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:134\u001b[0m, in \u001b[0;36mLightningOptimizer.__optimizer_step\u001b[0;34m(self, closure, profiler_name, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(profiler_name):\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_closure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:329\u001b[0m, in \u001b[0;36mAccelerator.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, lambda_closure, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m make_optimizer_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_optimizer_step(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, optimizer, opt_idx, lambda_closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_optimizer_step:\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_closure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_optimizer_step(optimizer, opt_idx)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mpost_optimizer_step(optimizer, opt_idx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:336\u001b[0m, in \u001b[0;36mAccelerator.run_optimizer_step\u001b[0;34m(self, optimizer, optimizer_idx, lambda_closure, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_optimizer_step\u001b[39m(\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m, optimizer: Optimizer, optimizer_idx: \u001b[38;5;28mint\u001b[39m, lambda_closure: Callable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    335\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_closure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_closure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:193\u001b[0m, in \u001b[0;36mTrainingTypePlugin.optimizer_step\u001b[0;34m(self, optimizer, lambda_closure, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer, lambda_closure: Callable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 193\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_closure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/torch/optim/adam.py:118\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 118\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    121\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py:732\u001b[0m, in \u001b[0;36mTrainLoop.run_training_batch.<locals>.train_step_and_backward_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step_and_backward_closure\u001b[39m():\n\u001b[0;32m--> 732\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step_and_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhiddens\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py:823\u001b[0m, in \u001b[0;36mTrainLoop.training_step_and_backward\u001b[0;34m(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;124;03m\"\"\"Wrap forward, zero_grad and backward in a closure so second order methods work\"\"\"\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step_and_backward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# lightning module hook\u001b[39;00m\n\u001b[0;32m--> 823\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhiddens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr_step_result \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip_backward \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/trainer/training_loop.py:290\u001b[0m, in \u001b[0;36mTrainLoop.training_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx, hiddens)\u001b[0m\n\u001b[1;32m    288\u001b[0m model_ref\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m Result()\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 290\u001b[0m     training_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpost_training_step()\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlogger_connector\u001b[38;5;241m.\u001b[39mcache_logged_metrics()\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:204\u001b[0m, in \u001b[0;36mAccelerator.training_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    201\u001b[0m args[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_device(args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mtrain_step_context():\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:155\u001b[0m, in \u001b[0;36mTrainingTypePlugin.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/multigrate/train/_trainingplans.py:63\u001b[0m, in \u001b[0;36mMultiVAETrainingPlan.training_step\u001b[0;34m(self, batch, batch_idx, optimizer_idx)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     62\u001b[0m     loss_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(kl_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkl_weight)\n\u001b[0;32m---> 63\u001b[0m     inference_outputs, _, scvi_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     loss \u001b[38;5;241m=\u001b[39m scvi_loss\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# fool classifier if doing adversarial training\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/scvi/train/_trainingplans.py:147\u001b[0m, in \u001b[0;36mTrainingPlan.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;124;03m\"\"\"Passthrough to `model.forward()`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/scvi/module/base/_decorators.py:32\u001b[0m, in \u001b[0;36mauto_move_data.<locals>.auto_transfer_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# decorator only necessary after training\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(p\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters()))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(device) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/scvi/module/base/_base_module.py:151\u001b[0m, in \u001b[0;36mBaseModuleClass.forward\u001b[0;34m(self, tensors, get_inference_input_kwargs, get_generative_input_kwargs, inference_kwargs, generative_kwargs, loss_kwargs, compute_loss)\u001b[0m\n\u001b[1;32m    149\u001b[0m generative_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerative(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerative_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerative_kwargs)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_loss:\n\u001b[0;32m--> 151\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerative_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inference_outputs, generative_outputs, losses\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/multigrate/module/_multivae_torch.py:422\u001b[0m, in \u001b[0;36mMultiVAETorch.loss\u001b[0;34m(self, tensors, inference_outputs, generative_outputs, kl_weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m masks \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]  \u001b[38;5;66;03m# [batch_size] * num_modalities\u001b[39;00m\n\u001b[1;32m    419\u001b[0m recon_loss, modality_recon_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_recon_loss(\n\u001b[1;32m    420\u001b[0m     xs, rs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses, integrate_on, size_factor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_coefs, masks\n\u001b[1;32m    421\u001b[0m )\n\u001b[0;32m--> 422\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m kl(\u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogvar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, Normal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_coefs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    425\u001b[0m     integ_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/torch/distributions/normal.py:54\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNormal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Theis/multigrate/venv/lib/python3.9/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 55\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28msuper\u001b[39m(Distribution, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (128, 15)) of distribution Normal(loc: torch.Size([128, 15]), scale: torch.Size([128, 15])) to satisfy the constraint Real(), but found invalid values:\ntensor([[ 0.2573,  0.1445, -0.2776,  ...,  0.2039,  0.2587,  0.2058],\n        [ 0.2999,  0.0161, -0.6274,  ...,  0.1477,  0.4026,  0.2286],\n        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n        ...,\n        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n        [ 0.3256,  0.1833, -0.0688,  ...,  0.1692,  0.2093,  0.1883]],\n       grad_fn=<MulBackward0>)"
     ]
    }
   ],
   "source": [
    "model.train(max_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a5550",
   "metadata": {},
   "source": [
    "Plot losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e004f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffad114",
   "metadata": {},
   "source": [
    "Integration loss is always 0 because we don't explicitely integrate the batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbd017",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1d4f8",
   "metadata": {},
   "source": [
    "Get the latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c7d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_latent_representation()\n",
    "combined = adata\n",
    "combined.write_h5ad(str(data_path / f'combined_multigrate_{file_suffix}.h5ad'))\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(combined, n_pcs=15, use_rep=\"latent\", metric=\"cosine\")\n",
    "sc.tl.umap(combined)\n",
    "sc.tl.leiden(combined, key_added=\"leiden_combined\", resolution=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f816a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cite = combined[combined.obs['group'] == 0, :] # adt and rna are combined by default because of the network architecture\n",
    "adata_cytof = combined[combined.obs['group'] == 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b5005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rand_score(adata_cytof.obs['leiden_combined'], adata_cytof.obs['major_cell_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rand_score(combined_cite.obs['leiden_combined'], combined_cite.obs['Annotation_major_subset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a818b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scib.metrics.silhouette(combined_cite, 'Annotation_major_subset',\n",
    "                        embed='latent'), scib.metrics.silhouette(combined_cite, 'leiden_combined',\n",
    "                                                                               embed='latent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scib.metrics.isolated_labels(combined_cite, 'Annotation_major_subset',\n",
    "                             embed='latent',\n",
    "                             batch_key='COMBAT_participant_timepoint_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46526ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scib.metrics.silhouette_batch(combined_cite, 'COMBAT_participant_timepoint_ID', 'Annotation_major_subset',\n",
    "                              'latent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb7f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "scib.metrics.graph_connectivity(combined_cite, 'Annotation_major_subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scib.metrics.silhouette(adata_cytof, 'major_cell_type',\n",
    "                        embed='latent'), scib.metrics.silhouette(adata_cytof, 'leiden_combined',\n",
    "                                                                               embed='latent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b926cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scib.metrics.isolated_labels(adata_cytof, 'major_cell_type',\n",
    "                             embed='latent',\n",
    "                             batch_key='COMBAT_participant_timepoint_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scib.metrics.silhouette_batch(adata_cytof, 'COMBAT_participant_timepoint_ID', 'major_cell_type', 'latent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526130e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scib.metrics.graph_connectivity(adata_cytof, 'major_cell_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    combined,\n",
    "    color=[\"leiden_combined\", \"group\", batch_key],\n",
    "    frameon=False,\n",
    "    ncols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40402cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sc.pl.embedding(combined_cite, \"X_umap\", color=[\"Annotation_major_subset\"], title=\"CITE cell type\",\n",
    "                      return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sc.pl.embedding(combined_cite, \"X_umap\", color=[\"leiden_combined\"], title=\"CITE cell type\",\n",
    "                      return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be77fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sc.pl.embedding(adata_cytof, \"X_umap\", color=[\"leiden_combined\"], title=\"Cytof cell type\",\n",
    "                      return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sc.pl.embedding(adata_cytof, \"X_umap\", color=[\"major_cell_type\"], title=\"Cytof cell type\",\n",
    "                      return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sc.pl.embedding(adata_cytof, \"X_umap\", color=[batch_key], title=\"Cytof bacth\",\n",
    "                      return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449865e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sc.pl.embedding(combined_cite, \"X_umap\", color=[batch_key], title=\"Combined CITE batch\",\n",
    "                      return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15d0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3004a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
